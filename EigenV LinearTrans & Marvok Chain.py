#!/usr/bin/env python
# coding: utf-8

1.
è§£é©¬å¯å¤«çŸ©é˜µæœ€ç»ˆç¨³æ€
2.
ç”¨é©¬å¯å¤«çŸ©é˜µå’ŒEigenvectoræ¥å»ºç«‹ç½‘é¡µè¿½è¸ªæ¨¡å‹

#Eigenvalues and Eigenvectors
# - apply linear transformations, eigenvalues and eigenvectors in a webpage navigation model
### Packages
import numpy as np
import matplotlib.pyplot as plt
import scipy.sparse.linalg


# For the sake of the example consider there are only a small number of pages 5 x 5. 
# Transformation Matrix= P, 5 x 5 çŸ©é˜µ
# Define vector X, åˆå§‹çŠ¶æ€ X0, Xn= P * Xn-1
ä¾‹ï¼š
P = np.array([ 
    
    [0, 0.75, 0.35, 0.25, 0.85], 
    [0.15, 0, 0.35, 0.25, 0.05], 
    [0.15, 0.15, 0, 0.25, 0.05], 
    [0.15, 0.05, 0.05, 0, 0.05], 
    [0.55, 0.05, 0.25, 0.25, 0]  
]) 

åˆå§‹çŠ¶æ€å‡å¦‚æ˜¯ä»ç¬¬å››ä¸ªç½‘é¡µå¼€å§‹ï¼š
X0 = np.array([[0],[0],[0],[1],[0]])
é‚£ä¹ˆç¬¬äºŒä¸ªçŠ¶æ€å°±æ˜¯ï¼š
# Multiply matrix P and X_0 (matrix multiplication).
X1 = P @ X0

print(f'Sum of columns of P: {sum(P)}')
print(f'X1:\n{X1}')

# Applying the transformation m times you can find a vector Xm with the probabilities of the browser being at each of the pages after m steps of navigation.

è¿­ä»£æˆ–å˜æ¢20æ¬¡
X = np.array([[0],[0],[0],[1],[0]])
m = 20

for t in range(m):
    X = P @ X
    
print(X)
# It is useful to predict the probabilities in Xm when m is large, and thus determine what pages a browser is more likely to visit after a long period of browsing the web. In other words, we want to know which pages ultimately get the most traffic. One way to do that is just apply the transformation many times, and with this small $5 \times 5$ example you can do that just fine. In real life problems, however, you'll have enormous matrices and doing so will be computationally expensive. Here is where eigenvalues and eigenvectors can help here significantly reducing the amount of calculations. Let's see how!
ä¸æ–­å¢åŠ mï¼Œm=50, m=5000, m=50000, æœ€åå‡ºç° Xm+1 = Xmï¼Œç³»ç»Ÿè¿›å…¥ç¨³æ€
è¿™ç§ç¨³æ€ï¼Œè¡¨ç¤ºä¸º: æœ‰ Vn+1 = Vn, æœ‰P * Vn = 1 * Vn+1ï¼Œåˆ™ Vnæˆ–Vn+1ä¸ºEigenvector with Eigenvalue = 1ã€‚ å³ç¨³æ€çš„Vnæ˜¯è½¬åŒ–çŸ©é˜µPçš„Eigenvectorä»¥Eigenvalueä¸º1ã€‚
###å¿…é¡»æ˜¯Markov Matrix æ‰æœ‰è¿™æ ·çš„æ€§è´¨###

# Begin by finding eigenvalues and eigenvectors for the previously defined matrix Pï¼š
eigenvals, eigenvecs = np.linalg.eig(P)
print(f'Eigenvalues of P:\n{eigenvals}\n\nEigenvectors of P\n{eigenvecs}')

# In general, a square matrix whose entries are all nonnegative, and the sum of the elements for each column is equal to 1 is called a Markov matrix. 
# Markov matrices have a handy property - they always have an eigenvalue equal to 1. 
# You can easily verify that the matrix P you defined earlier is in fact a Markov matrix. 
# So, if  ğ‘š is large enough, the equation  ğ‘‹ğ‘š=ğ‘ƒğ‘‹ğ‘šâˆ’1
  can be rewritten as  ğ‘‹ğ‘š=ğ‘ƒğ‘‹ğ‘šâˆ’1=1Ã—ğ‘‹ğ‘š
  This means that predicting probabilities at time  ğ‘š , when  ğ‘š is large you can simply just look for an eigenvector corresponding to the eigenvalue  1
# So, let's extract the eigenvector associated to the eigenvalue 1. 

X_inf = eigenvecs[:,0]

print(f"Eigenvector corresponding to the eigenvalue 1:\n{X_inf[:,np.newaxis]}")

Just to verify the results:
perform matrix multiplication  ğ‘ƒğ‘‹ (multiply matrix P and vector X_inf) to check that the result will be equal to the vector  ğ‘‹ (X_inf).

def check_eigenvector(P, X_inf):
    X_check = P @ X_inf
    return X_check

X_check = check_eigenvector(P, X_inf)
print("Original eigenvector corresponding to the eigenvalue 1:\n" + str(X_inf))
print("Result of multiplication:" + str(X_check))

# Function np.isclose compares two NumPy arrays element by element, allowing for error tolerance (rtol parameter).
print("Check that PX=X element by element:" + str(np.isclose(X_inf, X_check, rtol=1e-10)))

This result gives the direction of the eigenvector, but as you can see the entries can't be interpreted as probabilities 
since you have negative values, and they don't add to 1. 
That's no problem. Remember that 
by convention np.eig returns eigenvectors with norm 1, but actually any vector on the same line is also an eigenvector to the eigenvalue 1, 
so you can simply scale the vector so that all entries are positive and add to oneã€‚This will give you the long-run probabilities of landing on a given web pageã€‚


X_inf = X_inf/sum(X_inf)
print(f"Long-run probabilities of being at each webpage:\n{X_inf[:,np.newaxis]}")

å¯¹æœ€ç»ˆç¨³æ€çš„ç¿»è¯‘ï¼š
# This means that after navigating the web for a long time, 
the probability that the browser is at page 1 is 0.394, of being on page 2 is 0.134, on page 3 0.114, on page 4 0.085, and finally page 5 has a probability of 0.273.
# Looking at this result you can conclude that page 1 is the most likely for the browser to be at, while page 4 is the least probable one.
# If you compare the result of `X_inf` with the one you got after evolving the systems 20 times, they are the same up to the third decimal!
# Here is a fun fact: this type of a model was the foundation of the PageRank algorithm, which is the basis of Google's very successful search engine.

è¿™æ˜¯æœ€é‡è¦ä¸”æµè¡Œçš„ç½‘é¡µè¿½è¸ªç®—æ³•çš„åŸºç¡€ï¼ï¼ï¼July/2024
