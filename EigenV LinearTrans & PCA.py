

###Application of Eigenvalues and Eigenvectors: Principal Component Analysis (PCA)
# Packagesï¼š
import numpy as np
import matplotlib.pyplot as plt
import scipy.sparse.linalg

ç®—æ³•ï¼Œæµç¨‹æ¦‚è¿°ï¼š
#To apply PCA on any dataset 
#you will begin by defining the covariance matrix. 
#After that you will compute the eigenvalues and eigenvectors of this covariance matrix. 
#Each of these eigenvectors will be a â€œprincipal componentâ€. 
#To perform the dimensionality reduction, you will take the k principal components associated to the k biggest eigenvalues, 
#and transform the original data by projecting it onto the direction of these principal components (eigenvectors).


###ç¬¬ä¸€æ­¥ï¼šLoad the dataå¯¼å…¥å’Œã€é€†å¯è§†åŒ–ã€‘å›¾åƒè¿›å…¥ä¸€ä¸ª Dataset Arrayï¼Œä¾¿äºè¿›è¡Œåç»­æ“ä½œï¼ˆPCAï¼‰ã€‚

#Populationæ˜¯ â€˜Cat and dog faceâ€™ dataset from Kaggle. æˆ‘ä»¬å¤„ç†çš„Datasetæ˜¯å…¶ä¸­çš„ Cat Faceã€‚
# Begin by loading the images and transforming them to black and white using `load_images` function from utils. 
imgs = utils.load_images('./data/')

# imgs å°±æ˜¯å›¾åƒçš„datasetï¼Œæ¯ä¸ªè§‚æµ‹ç‚¹å°±æ˜¯ä¸€å¼ å›¾åƒ(çŒ«å¤´)ï¼Œè¿™é‡Œæ¯ä¸ªå›¾åƒå°±æ˜¯ä¸€ä¸ªçŸ©é˜µMatrixï¼ŒçŸ©é˜µçš„æ¯ä¸ªelementå°±æ˜¯ä¸€ä¸ªpixelåƒç´ ç‚¹ã€‚

#å¦‚ä¸‹ä»£ç æ“ä½œï¼Œå¯ä»¥æŸ¥çœ‹è¿™é‡Œå…±æœ‰å‡ å¼ å›¾è±¡ï¼Œæ¯å¼ å›¾ç‰‡æ˜¯ n x nçš„çŸ©é˜µ (ä»€ä¹ˆæ ·çš„çŸ©é˜µå½¢çŠ¶Shape)ã€‚

height, width = imgs[0].shape
print(f'\nYour dataset has {len(imgs)} images of size {height}x{width} pixels\n')
# ç»“æœï¼šYour dataset has 55 images of size 64x64 pixels
#å…±æœ‰55å¼ å›¾ç‰‡åœ¨è¿™ä¸ªdatasetï¼Œæ¯å¼ å›¾ç‰‡éƒ½æ˜¯ Matrix 64 x 64ï¼Œå³æˆ‘ä»¬é€šå¸¸è¯´çš„ â€œ64åƒç´ â€ã€‚

# Go ahead and plot one image to see what they look like. You can use the colormap 'gray' to plot in black and white. 
#æ”¹å˜å…¶ä¸­imgs[]ä¸­çš„å‚æ•°æ¥æŸ¥çœ‹datasetä¸­çš„ä¸åŒå›¾åƒï¼ˆçŸ©é˜µï¼‰ã€‚
plt.imshow(imgs[0], cmap='gray')


# When working with images, you can consider each pixel as a variable. æ¯ä¸ªå›¾ç‰‡æ˜¯ 64 x 64Matrix æ„å‘³ç€æ€»å…±64ä¸ªå˜é‡ã€‚
# æˆ‘ä»¬ç°åœ¨è¦æ“ä½œè¿™ä¸ªå«æœ‰64ä¸ªå˜é‡Xiï¼Œå’Œ55ä¸ªè§‚æµ‹ç‚¹çš„ Datasetã€‚é¦–å…ˆè¦æŠŠimageå˜æˆä¸€èˆ¬çš„ç»Ÿè®¡æ•°æ®å½¢å¼ã€‚ä¹Ÿå¯ä»¥ç§°ä¸ºã€é€†å¯è§†åŒ–ã€‘ã€‚
# Having each image in matrix form is good for visualizing the image, but not so much for operating on each variable. 
# In order to apply PCA for dimensionality reduction 
# You will need to flatten each image into a single row vector. You can do this using the `reshape` function from NumPy. æ“ä½œä»£ç å¦‚ä¸‹ï¼š

imgs_flatten = np.array([im.reshape(-1) for im in imgs])

print(f'imgs_flatten shape: {imgs_flatten.shape}')

# The resulting array will have 55 rows, one for each image, and 64x64=4096 columns.
#ç°åœ¨å˜æˆäº†ä¸€ä¸ª 55 x 4096çš„å¤§Arrayï¼Œä½œä¸ºæˆ‘ä»¬çš„æ•°æ®é›† Dataset Arrayï¼Œæ–¹ä¾¿æˆ‘ä»¬æ–½åŠ æ“ä½œï¼Œå¦‚PCAå˜æ¢ã€‚



###ç¬¬äºŒæ­¥ï¼š æ‰¾åˆ°åæ–¹å·®çŸ©é˜µ Get the covariance matrixï¼š
# Now that you have the images in the correct shape you are ready to apply PCA on the flattened dataset. 
# If you consider each pixel (column) as a variable, and each image (rows) as an obervation you will have 55 observations of 4096 variable
#æ•°æ®é›†ï¼š æ¯ä¸ªåƒç´ ç‚¹ï¼ˆcolumnï¼‰ä½œä¸ºä¸€ä¸ªå˜é‡Xiï¼Œæ¯å¼ å›¾ç‰‡ï¼ˆrowï¼‰ä½œä¸ºä¸€ä¸ªè§‚æµ‹ç‚¹Oservationï¼Œè¯¥æ•°æ®Arrayå…±æœ‰55ä¸ªè§‚æµ‹ç‚¹ï¼Œ4096ä¸ªå˜é‡ã€‚

ç°åœ¨ç¬¬ä¸€æ­¥æ˜¯ï¼šã€ä¸­å¿ƒåŒ–ã€‘Xi - meanXi æ‰¾åˆ° Dataset Arrayçš„ä¸­å¿ƒåŒ–çŸ©é˜µCentered Matrixã€‚
# In order to get the covariance matrix you first need to center the data by subtracting the mean for each variable (column). 
åˆ©ç”¨ä¸‹é¢ä¸‰å¤§å…¬å¼Functionsåšåˆ°è¿™ä¸€ç‚¹ï¼š
np.mean: use this function to compute the mean of each variable, just remember to pass the correct axis argument.
np.repeat: This will allow for you to repeat the values of each  ğœ‡ğ‘–
np.reshape: Use this function to reshape the repeated values into a matrix of shape the same size as your input data. To get the correct matrix after the reshape, remember to use the parameter order='F'.

def center_data(Y):
    """
    Center your original data
    Args:
         Y (ndarray): input data. Shape (n_observations x n_pixels)
    Outputs:
        X (ndarray): centered data
    """
    mean_vector = np.mean(Y, axis=0)
    mean_matrix = np.repeat(mean_vector,Y.shape[0],axis=0)
    # use np.reshape to reshape into a matrix with the same size as Y. Remember to use order='F'
    mean_matrix = np.reshape(mean_matrix,Y.shape,order='F')
    
    X = Y - mean_matrix
    return X
    
è¿™é‡Œçš„Xå°±æ˜¯ä¸­å¿ƒåŒ–çŸ©é˜µï¼ŒDataset Arrayçš„Centered Matrixï¼Œå®ƒåº”è¯¥å’ŒDataset Arrayï¼Œ Y æœ‰ä¸€æ ·çš„å½¢çŠ¶Shpaeï¼Œæ­£å¦‚Reshape Functionåšåˆ°çš„ã€‚
#æ³¨æ„ axis=0æˆ–axis=1 å¯¹è¡Œï¼Œå¯¹åˆ—çš„å‚æ•°è®¾ç½®ã€‚ä»¥åŠ X.shape[0 æˆ– 1]æˆ–è€…X.shapeæ¥å¾—åˆ°çŸ©é˜µXçš„è¡Œæ•°ï¼Œåˆ—æ•°ï¼Œå’Œè¡Œåˆ—æ•°çš„ä»£æ•°å€¼ã€‚

# Go ahead and apply the `center_data` function to your data in `imgs_flatten`. 
# You can also print the image again and check that the face of the cat still looks the same. 
# This is because the color scale is not fixed, but rather relative to the values of the pixels. 

X = center_data(imgs_flatten)
plt.imshow(X[0].reshape(64,64), cmap='gray')

ç¬¬äºŒæ­¥ï¼šç”¨ã€ä¸­å¿ƒåŒ–ã€‘çŸ©é˜µæ‰¾åˆ°ã€Covariance Matrixã€‘åæ–¹å·®çŸ©é˜µ
å…¬å¼ï¼šC = ï¼ˆ1/n-1ï¼‰* (Xè½¬ç½® * X)   Xæ˜¯centered matrix
ç‚¹ä¹˜  np.dot(X1 ,X2 )
è½¬ç½®  np.transpose(X)
# Now that you have your centered data, X, you can go ahead and find the covariance matrix 
# The covariance matrix can be found by appliying the dot product between np.transpose(X) and X, and divide by the number of observations minus 1.
# To perform the dot product you can simply use the function np.dot()

def get_cov_matrix(X):
    """ Calculate covariance matrix from centered data X
    Args:
        X (np.ndarray): centered data matrix
    Outputs:
        cov_matrix (np.ndarray): covariance matrix
    """
    cov_matrix = np.dot(np.transpose(X),  X)
    cov_matrix = cov_matrix/(X.shape[0]-1)
    
    return cov_matrix   

cov_matrix = get_cov_matrix(X)
# Check the dimensions of the covariance matrix, it should be a square matrix with 4096 rows and columns. 
print(f'Covariance matrix shape: {cov_matrix.shape}') 
ç»“æœï¼š4096 x 4096


###ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—åæ–¹å·®çŸ©é˜µCçš„Eigenvalueså’ŒEigenvectorsã€‚Compute the eigenvalues and eigenvectorsï¼š

# Now you are all set to compute the eigenvalues and eigenvectors of the covariance matrix.
# Due to performance constaints, you will not be using å…¬å¼ï¼š np.linalg.eig 
# But rather the very similar function å…¬å¼ï¼š scipy.sparse.linalg.eigsh
# This function allows you to compute fewer number of eigenvalue-eigenvector pairs. è¿ç®—ç»æµï¼Œé€‚ç”¨äºå°å‹è®¡ç®—æœº

# it can be shown that at most 55 eigenvalues of C will be different from zero, which is the smallest dimension of the data matrix X. 
# Thus, for computational efficiency, you will only be computing the first biggest 55 Eigenvalues
# and their corresponding Eigenvectors
# Feel free to try changing the å‚æ•° â€œkâ€ parameter in å…¬å¼ â€œscipy.sparse.linalg.eigshâ€ to something slightly bigger, 
# to verify that all the new eigenvalues are zero. Try to keep it below 80, otherwise it will take too long to compute. 
# The outputs of this scipy function are exactly the same as the ones from `np.linalg.eig`, except eigenvalues are ordered in decreasing order, so if you want to check out the largest eigenvalue you need to look into the last position of the vector. 

scipy.random.seed(7)
eigenvals, eigenvecs = scipy.sparse.linalg.eigsh(cov_matrix, k=55)
print(f'Ten largest eigenvalues: \n{eigenvals[-10:]}')

ä¿æŒæ¯æ¬¡è¿ç®—EigenVectoræ–¹å‘çš„ä¸€è‡´æ€§ï¼šç”¨scipy.random.seed(7)
# The random seed is fixed in the code above to help ensure the same eigenvectors are calculated each time. 
# This is because for each eigenvector, there are actually two possible outcomes with norm 1. They fall on the same line but point in opposite directions. 
# Both possibilities are correct, but by fixing the seed you guarantee you will always get the same result. 
# In order to get a consistent result with å…¬å¼ï¼šâ€œnp.linalg.eigâ€, you will invert the order of `eigenvals` and `eigenvecs`
# so they are both ordered from largest to smallest eigenvalue.

eigenvals = eigenvals[::-1]
eigenvecs = eigenvecs[:,::-1]

print(f'Ten largest eigenvalues: \n{eigenvals[:10]}')


# Each of the eigenvectors you found will represent one principal component. 
# The eigenvector associated with the largest eigenvalue will be the first principal component, 
# the eigenvector associated with the second largest eigenvalue will be the second principal component, and so on. 
# It is pretty interesting to see that each principal component usually extracts some relevant features, or patterns from each image. 
# In the next cell you will be visualizing the first sixteen components to see thatï¼š

fig, ax = plt.subplots(4,4, figsize=(20,20))
for n in range(4):
    for k in range(4):
        ax[n,k].imshow(eigenvecs[:,n*4+k].reshape(height,width), cmap='gray')
        ax[n,k].set_title(f'component number {n*4+k+1}')


# What can you say about each of the principal components? 

# <a name='2.4'></a>
# ### 2.4 Transform the centered data with PCA
# 
# Now that you have the first 55 eigenvalue-eivenvector pairs, you can transform your data to reduce the dimensions. Remember that your data originally consisted of 4096 variables. Suppose you want to reduce that to just 2 dimensions, then all you need to do to perform the reduction with PCA is take the dot product between your centered data and the matrix $\boldsymbol{V}=\begin{bmatrix} v_1 & v_2 \end{bmatrix}$, whose columns are the first 2 eigenvectors, or principal components, associated to the 2 largest eigenvalues.
# 
# <a name='ex03'></a>
# ### Exercise 5
# In the next cell you will define a function that, given the data matrix, the eigenvector matrix (always sorted according to decreasing eignevalues), and the number of principal components to use, performs PCA.

# In[337]:


# GRADED cell
def perform_PCA(X, eigenvecs, k):
    """
    Perform dimensionality reduction with PCA
    Inputs:
        X (ndarray): original data matrix. Has dimensions (n_observations)x(n_variables)
        eigenvecs (ndarray): matrix of eigenvectors. Each column is one eigenvector. The k-th eigenvector 
                            is associated to the k-th eigenvalue
        k (int): number of principal components to use
    Returns:
        Xred
    """
    
    ### START CODE HERE ###
    V = eigenvecs[:,:k]
    Xred = np.dot(X, V)
    ### END CODE HERE ###
    return Xred
    # grade-up-to-here


# Try out this function, reducing your data to just two components

# In[338]:


Xred2 = perform_PCA(X, eigenvecs,2)
print(f'Xred2 shape: {Xred2.shape}')


# In[339]:


# Test your solution.
w4_unittest.test_check_PCA(perform_PCA)


# <a name='2.5'></a>
# ### 2.5 Analyzing the dimensionality reduction in 2 dimensions
# 
# One cool thing about reducing your data to just two components is that you can clearly visualize each cat image on the plane. Remember that each axis on this new plane represents a linear combination of the original variables, given by the direction of the two eigenvectors.
# 
# Use the function `plot_reduced_data` in `utils`to visualize the transformed data. Each blue dot represents an image, and the number represents the index of the image. This is so you can later recover which image is which, and gain some intuition.

# In[340]:


utils.plot_reduced_data(Xred2)


# If two points end up being close to each other in this representation, it is expected that the original pictures should be similar as well. 
# Let's see if this is true. Consider for example the images 19, 21 and 41, which appear close to each other on the top center of the plot. Plot the corresponding cat images vertfy that they correspond to similar cats. 

# In[341]:


fig, ax = plt.subplots(1,3, figsize=(15,5))
ax[0].imshow(imgs[19], cmap='gray')
ax[0].set_title('Image 19')
ax[1].imshow(imgs[21], cmap='gray')
ax[1].set_title('Image 21')
ax[2].imshow(imgs[41], cmap='gray')
ax[2].set_title('Image 41')
plt.suptitle('Similar cats')


# As you can see, all three cats have white snouts and black fur around the eyes, making them pretty similar.
# 
# Now, let's choose three images that seem far appart from each other, for example image 18, on the middle right, 41 on the top center and 51 on the lower left, and also plot the images

# In[342]:


fig, ax = plt.subplots(1,3, figsize=(15,5))
ax[0].imshow(imgs[18], cmap='gray')
ax[0].set_title('Image 18')
ax[1].imshow(imgs[41], cmap='gray')
ax[1].set_title('Image 41')
ax[2].imshow(imgs[51], cmap='gray')
ax[2].set_title('Image 51')
plt.suptitle('Different cats')


# In this case, all three cats look really different, one being completely black, another completely white, and the the third one a mix of both colors.
# 
# 
# Feel free to choose different pairs of points and check how similar (or different) the pictures are. 
# 
# <a name='2.6'></a>
# ### 2.6 Reconstructing the images from the eigenvectors
# 
# When you compress the images using PCA, you are losing some information because you are using fewer variables to represent each observation. 
# 
# A natural question arises: how many components do you need to get a good reconstruction of the image? Of course, what determines a "good" reconstruction might depend on the application.
# 
# A cool thing is that with a simple dot product you can transform the data after applying PCA back to the original space. This means that you can reconstruct the original image from the transformed space and check how distorted it looks based on the number of components you kept.
# 
# Suppose you obtained the matrix $X_{red}$ by keeping just two eigenvectors, then $X_{red} = \mathrm{X}\underbrace{\left[v_1\  v_2\right]}_{\boldsymbol{V_2}}$.
# 
# To transform the images back to the original variables space all you need to do is take the dot product between $X_{red}$ and $\boldsymbol{V_2}^T$. If you were to keep more components, say $k$, then simply replace $\boldsymbol{V_2}$ by $\boldsymbol{V_k} = \left[v_1\ v_2\ \ldots\ v_k\right]$. Notice that you can't make any combination you like, if you reduced the original data to just $k$ components, then the recovery must consider only the first $k$ eigenvectors, otherwise you will not be able to perform the matrix multiplication.
# 
# In the next cell you will define a function that given the transformed data $X_{red}$ and the matrix of eigenvectors returns the recovered image. 

# In[343]:


def reconstruct_image(Xred, eigenvecs):
    X_reconstructed = Xred.dot(eigenvecs[:,:Xred.shape[1]].T)

    return X_reconstructed


# Let's see what the reconstructed image looks like for different number of principal components

# In[344]:


Xred1 = perform_PCA(X, eigenvecs,1) # reduce dimensions to 1 component
Xred5 = perform_PCA(X, eigenvecs, 5) # reduce dimensions to 5 components
Xred10 = perform_PCA(X, eigenvecs, 10) # reduce dimensions to 10 components
Xred20 = perform_PCA(X, eigenvecs, 20) # reduce dimensions to 20 components
Xred30 = perform_PCA(X, eigenvecs, 30) # reduce dimensions to 30 components
Xrec1 = reconstruct_image(Xred1, eigenvecs) # reconstruct image from 1 component
Xrec5 = reconstruct_image(Xred5, eigenvecs) # reconstruct image from 5 components
Xrec10 = reconstruct_image(Xred10, eigenvecs) # reconstruct image from 10 components
Xrec20 = reconstruct_image(Xred20, eigenvecs) # reconstruct image from 20 components
Xrec30 = reconstruct_image(Xred30, eigenvecs) # reconstruct image from 30 components

fig, ax = plt.subplots(2,3, figsize=(22,15))
ax[0,0].imshow(imgs[21], cmap='gray')
ax[0,0].set_title('original', size=20)
ax[0,1].imshow(Xrec1[21].reshape(height,width), cmap='gray')
ax[0,1].set_title('reconstructed from 1 components', size=20)
ax[0,2].imshow(Xrec5[21].reshape(height,width), cmap='gray')
ax[0,2].set_title('reconstructed from 5 components', size=20)
ax[1,0].imshow(Xrec10[21].reshape(height,width), cmap='gray')
ax[1,0].set_title('reconstructed from 10 components', size=20)
ax[1,1].imshow(Xrec20[21].reshape(height,width), cmap='gray')
ax[1,1].set_title('reconstructed from 20 components', size=20)
ax[1,2].imshow(Xrec30[21].reshape(height,width), cmap='gray')
ax[1,2].set_title('reconstructed from 30 components', size=20)


# As you can see, as the number of components increases, the reconstructed image looks more and more as the original one. Even with as little as 1 component you can are least identify where the relevant features such as eyes and nose are located. 
# 
# What happens when you consider all of the 55 eigenvectors associated to non-zero eigenvalues? Go ahead and experiment with different number of principal components and see what happens.

# <a name='2.7'></a>
# ### 2.7 Explained variance
# 
# When deciding how many components to use for the dimensionality reduction, one good criteria to consider is the explained variance. 
# 
# The explained variance is measure of how much variation in a dataset can be attributed to each of the principal components (eigenvectors). In other words, it tells us how much of the total variance is â€œexplainedâ€ by each component. 
# 
# In PCA, the first principal component, i.e. the eigenvector associated to the largest eigenvalue, is the one with greatest explained variance. As you might remember from the lectures, the goal of PCA is to reduce the dimensionality by projecting data in the directions with biggest variability. 
# 
# In practical terms, the explained variance of a principal component is the ratio between its associated eigenvalue and the sum of all the eigenvalues. So, for our example, if you want the explained variance of the first principal component you will need to do $\frac{\lambda_1}{\sum_{i=1}^{55} \lambda_i}$
# 
# Next, let's plot the explained variance of each of the 55 principal components, or eigenvectors. Don't worry about the fact that you only computed 55 eigenvalue-eigenvector pairs, recall that all the remaining eigenvalues of the covariance matrix are zero, and thus won't add enything to the explained variance.
# 

# In[345]:


explained_variance = eigenvals/sum(eigenvals)
plt.plot(np.arange(1,56), explained_variance)


# As you can see, the explained variance falls pretty fast, and is very small after the 20th component.
# 
# A good way to decide on the number of components is to keep the ones that explain a very high percentage of the variance, for example 95%. 
# 
# For an easier visualization you can plot the cumulative explained variance. You can do this with the `np.cumsum` function. Let's see what this looks like

# In[346]:


explained_cum_variance = np.cumsum(explained_variance)
plt.plot(np.arange(1,56), explained_cum_variance)
plt.axhline(y=0.95, color='r')


# In red you can see the 95% line. This means that if you want to be able to explain 95% of the variance of your data you need to keep 35 principal components. 
# 
# Let's see how some of the original images look after the reconstruction when using 35 principal components 
# 
# 

# In[347]:


Xred35 = perform_PCA(X, eigenvecs, 35) # reduce dimensions to 35 components
Xrec35 = reconstruct_image(Xred35, eigenvecs) # reconstruct image from 35 components

fig, ax = plt.subplots(4,2, figsize=(15,28))
ax[0,0].imshow(imgs[0], cmap='gray')
ax[0,0].set_title('original', size=20)
ax[0,1].imshow(Xrec35[0].reshape(height, width), cmap='gray')
ax[0,1].set_title('Reconstructed', size=20)

ax[1,0].imshow(imgs[15], cmap='gray')
ax[1,0].set_title('original', size=20)
ax[1,1].imshow(Xrec35[15].reshape(height, width), cmap='gray')
ax[1,1].set_title('Reconstructed', size=20)

ax[2,0].imshow(imgs[32], cmap='gray')
ax[2,0].set_title('original', size=20)
ax[2,1].imshow(Xrec35[32].reshape(height, width), cmap='gray')
ax[2,1].set_title('Reconstructed', size=20)

ax[3,0].imshow(imgs[54], cmap='gray')
ax[3,0].set_title('original', size=20)
ax[3,1].imshow(Xrec35[54].reshape(height, width), cmap='gray')
ax[3,1].set_title('Reconstructed', size=20)


# Most of these reconstructions look pretty good, and you were able to save a lot of memory by reducing the data from 4096 variables to just 35!
# 
# Now that you understand how the explained variance works you can play around with different amount of explained variance and see how this affects the reconstructed images. You can also explore how the reconstruction for different images looks. 
# 
# As you can see, PCA is a really useful tool for dimensionality reduction. In this assignment you saw how it works on images, but you can apply the same principle to any tabular dataset. 
# 
# Congratulations! You have finished the assignment in this week.

# 
